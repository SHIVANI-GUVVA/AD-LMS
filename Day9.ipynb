{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load stopwords and initialize tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    tokens = simple_preprocess(text, deacc=True)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    return {\n",
        "        \"original_tokens\": tokens,\n",
        "        \"stemmed_tokens\": stemmed_tokens,\n",
        "        \"lemmatized_tokens\": lemmatized_tokens,\n",
        "    }\n",
        "\n",
        "# Load sample text from a file or use default text\n",
        "try:\n",
        "    with open(\"sample_text.txt\", \"r\") as file:\n",
        "        text = file.read()\n",
        "except FileNotFoundError:\n",
        "    text = \"Natural Language Processing enables machines to process human language effectively.\"\n",
        "\n",
        "# Preprocess the text\n",
        "processed_data = preprocess_text(text)\n",
        "\n",
        "# Print results\n",
        "print(\"Original Tokens:\")\n",
        "print(processed_data[\"original_tokens\"])\n",
        "\n",
        "print(\"\\nStemmed Tokens:\")\n",
        "print(processed_data[\"stemmed_tokens\"])\n",
        "\n",
        "print(\"\\nLemmatized Tokens:\")\n",
        "print(processed_data[\"lemmatized_tokens\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCi_6Lyb5UeL",
        "outputId": "95d357c8-3a70-4e02-a683-556c2d461185"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens:\n",
            "['natural', 'language', 'processing', 'enables', 'machines', 'process', 'human', 'language', 'effectively']\n",
            "\n",
            "Stemmed Tokens:\n",
            "['natur', 'languag', 'process', 'enabl', 'machin', 'process', 'human', 'languag', 'effect']\n",
            "\n",
            "Lemmatized Tokens:\n",
            "['natural', 'language', 'processing', 'enable', 'machine', 'process', 'human', 'language', 'effectively']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ufJbAsgt61PN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}